---
title: "Civil_War_Diaries"
name: Addison Horton
---

## Confederate Women Diaries

```{r}
library(tidyverse)
library(tidytext) 
library(readtext)
library(widyr)
library(SnowballC)
library(tidytext)
library(tidyverse)
library(readtext)
library(tm)
library(topicmodels)
library(readr)
```

```{r}
#zip file of all the .txt files. One for each issue. 
#download.file("https://github.com/dseefeldt/IndianaNewspapers/raw/main/bb-txt.zip", "bb-txt.zip")
#unzip("bb-txt.zip")

#diary.files <- read.csv("Files-Grid view.csv")

#zip file of all the .txt files. One for each issue. 
unzip("diary-txt.zip")


```

```{r}
# Metadata that includes info about each article.
diary.metadata <- read.csv("Meta-Grid view.csv")
```

```{r}

#txt.files <- readtext(file.path("/txt""))
#txt.files <- txt.files %>% 
  #arrange(doc_id)
file_paths <- list.files("text files/")
diary_texts <- readtext(paste("text files/", "*.txt", sep = ""))

diary_whole <- full_join(diary.metadata, diary_texts, by = c("file_name" = "doc_id")) %>% as_tibble()

#data_dir_diary <- paste(getwd(), "/txt", sep = "")
#diary_files <- readtext(paste0(data_dir_diary, "/*.txt"))

#read_delim(diary.files, col_names = txt_files)

#merged.diary <- merge(diary_files, diary.metadata, by="file_name", all=TRUE)

#diary_whole <- 
#  diary.metadata %>%  
 # arrange(file_name) %>% # sort metadata
 # bind_cols(diary.files) %>% # combine with texts
 # as_tibble() # convert to tibble for better screen viewing

```

```{r}

stop_words_custom <- stop_words %>% add_row(word="january", lexicon="NA") %>% add_row(word="february", lexicon="NA") %>% add_row(word="march", lexicon="NA") %>% add_row(word="april", lexicon="NA") %>% add_row(word="may", lexicon="NA") %>% add_row(word="june", lexicon="NA") %>% add_row(word="july", lexicon="NA") %>% add_row(word="august", lexicon="NA") %>% add_row(word="septmeber", lexicon="NA") %>% add_row(word="october", lexicon="NA") %>% add_row(word="november", lexicon="NA") %>% add_row(word="december", lexicon="NA")  %>% add_row(word="day", lexicon="NA") %>% add_row(word="december", lexicon="NA")  %>% add_row(word="morning", lexicon="NA") %>% add_row(word="december", lexicon="NA") %>% add_row(word="evening", lexicon="NA") %>% add_row(word="december", lexicon="NA") %>% add_row(word="miriam", lexicon="NA") %>% add_row(word="december", lexicon="NA") %>% add_row(word="miriam", lexicon="NA") %>% add_row(word="home", lexicon="NA") %>% add_row(word="phil", lexicon="NA") %>% add_row(word="chesnut", lexicon="NA") %>% add_row(word="willie", lexicon="NA") %>% add_row(word="mary", lexicon="NA") %>% add_row(word="yesterday", lexicon="NA") %>% add_row(word="time", lexicon="NA") 
#
diary_whole_unnest <- diary_whole %>% 
  unnest_tokens(word, text)  %>% 
 filter(str_detect(word, "[a-z']$"))  %>%
  anti_join(stop_words_custom)# eliminates stop words 

 #diary_whole_unnest <- diary_whole_unnest %>% filter(!grepl('[0-9]', word))


#diary.whole.n.count <- diary_whole_tidy %>%
#  count(word, sort = TRUE)
```

```{r}
diary.dtm <- diary_whole_unnest %>% 
  count(file_name, word) %>% 
  cast_dtm(file_name, word, n)
```

```{r}
diary.lda <- LDA(diary.dtm, k = 7, control = list(seed = 12345))
diary.lda

diary.topics <- tidy(diary.lda, matrix = "beta")
head(diary.topics)

diary.top.terms <- diary.topics %>%
  arrange(desc(beta)) %>% 
  group_by(topic) %>% slice(1:7)
```

```{r}
diary.top.terms %>%
  mutate(term = reorder_within(term, beta, topic)) %>%
  ggplot(aes(beta, term, fill = factor(topic))) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~ topic, scales = "free") +
  scale_y_reordered() 

```

```{r}
diary.lda <- LDA(diary.dtm, k = 17, control = list(seed = 12345))
diary.lda

diary.topics <- tidy(diary.lda, matrix = "beta")
head(diary.topics)

diary.top.terms <- diary.topics %>%
  arrange(desc(beta)) %>% 
  group_by(topic) %>% slice(1:5)


diary.top.terms %>%
  mutate(term = reorder_within(term, beta, topic)) %>%
  ggplot(aes(beta, term, fill = factor(topic))) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~ topic, scales = "free") +
  scale_y_reordered()
```

```{r}
diary_whole_unnest %>%
  count(word, sort = TRUE)
```

```{r}
diary_whole_unnest %>%
  filter(word %in% c("war", "peace")) %>% 
  count(year, word)
```

```{r}
library(ggplot2)

diary_whole_unnest %>%
  filter(word %in% c("war", "peace")) %>% 
  count(year, word) %>% 
  ggplot(aes(year, n, fill = word)) +
    geom_col(position = "fill")
```

```{r}
diary_whole_unnest %>%
  count(month, year, file_name)  %>% 
  group_by(month, year) %>% 
  summarize(avg_words = mean(n)) %>% 
  arrange(desc(avg_words))
```

```{r}
diary_whole_freq <- diary_whole_unnest %>%
  count(file_name, word, sort = T)  %>%# count occurrence of word and sort descending
  group_by(file_name) %>% 
  mutate(n_tot = sum(n), # count total number of words per doc
         term_freq = n/n_tot)
```

```{r}
diary_whole_unnest %>%
  count(name, word)  %>%  # count n for each word
  group_by(name) %>% 
  mutate(n_tot = sum(n), # count total number of words per doc
         term_freq = n/n_tot) %>% 
  arrange(desc(term_freq)) %>% # sort by term frequency
  top_n(1) %>%  # take the top for each president
  print(n = Inf) # print all rows
```

```{r}
diary_whole_unnest %>%
  count(state, word)  %>%  # count n for each word
  group_by(state) %>% 
  mutate(n_tot = sum(n), # count total number of words per doc
         term_freq = n/n_tot) %>% 
  arrange(desc(term_freq)) %>% # sort by term frequency
  top_n(1) %>%  # take the top for each president
  print(n = Inf) # print all rows
```

```{r}
diary_whole_unnest %>%
  count(year, word)  %>%  # count n for each word
  group_by(year) %>% 
  mutate(n_tot = sum(n), # count total number of words per doc
         term_freq = n/n_tot) %>% 
  arrange(desc(term_freq)) %>% # sort by term frequency
  top_n(1) %>%  # take the top for each president
  print(n = Inf) # print all rows
```

```{r}
diary_whole_unnest %>%
  filter(word %in% c("god", "yankees")) %>% 
  count(year, word) %>% 
  ggplot(aes(year, n, fill = word)) +
    geom_col(position = "fill")
```

```{r}
diary_whole_unnest %>%
  filter(word %in% c("lord", "yankees")) %>% 
  count(year, word) %>% 
  ggplot(aes(year, n, fill = word)) +
    geom_col(position = "fill")
```

```{r}
diary_whole_unnest %>%
  filter(word %in% c("poor", "hope")) %>% 
  count(year, word) %>% 
  ggplot(aes(year, n, fill = word)) +
    geom_col(position = "fill")
```

```{r}
diary_whole_unnest %>%
  count(month, year, word)  %>%  # count n for each word
  group_by(month) %>% 
  mutate(n_tot = sum(n), # count total number of words per doc
         term_freq = n/n_tot) %>% 
  arrange(desc(term_freq)) %>% # sort by term frequency
  top_n(1) %>%  # take the top for each president
  print(n = Inf) # print all rows
```

```{r}
library(SnowballC)
diary_whole_unnest_stem <- diary_whole_unnest %>%
        mutate(word_stem = wordStem(word))
```

```{r}
diary.lda <- LDA(diary.dtm, k = 20, control = list(seed = 12345))
diary.lda

#10 to 12 range - add miss mrs mister to stopwords - before look at miss vs mrs
#10 or less, better specificity 

diary.topics <- tidy(diary.lda, matrix = "beta")
head(diary.topics)

diary.top.terms <- diary.topics %>%
  arrange(desc(beta)) %>% 
  group_by(topic) %>% slice(1:5)


diary.top.terms %>%
  mutate(term = reorder_within(term, beta, topic)) %>%
  ggplot(aes(beta, term, fill = factor(topic))) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~ topic, scales = "free") +
  scale_y_reordered()
```

```{r}
diary_pre_1863 <- diary_whole_unnest %>% filter(year < 1863)

diary.dtm <- diary_pre_1863 %>% 
  count(file_name, word) %>% 
  cast_dtm(file_name, word, n)

diary.lda <- LDA(diary.dtm, k = 20, control = list(seed = 12345))
diary.lda

diary.topics <- tidy(diary.lda, matrix = "beta")
head(diary.topics)

diary.top.terms <- diary.topics %>%
  arrange(desc(beta)) %>% 
  group_by(topic) %>% slice(1:5)


diary.top.terms %>%
  mutate(term = reorder_within(term, beta, topic)) %>%
  ggplot(aes(beta, term, fill = factor(topic))) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~ topic, scales = "free") +
  scale_y_reordered()
```

```{r}
diary_post_1863 <- diary_whole_unnest %>% filter(year >= 1863)

diary.dtm <- diary_post_1863 %>% 
  count(file_name, word) %>% 
  cast_dtm(file_name, word, n)

diary.lda <- LDA(diary.dtm, k = 20, control = list(seed = 12345))
diary.lda

diary.topics <- tidy(diary.lda, matrix = "beta")
head(diary.topics)

diary.top.terms <- diary.topics %>%
  arrange(desc(beta)) %>% 
  group_by(topic) %>% slice(1:5)


diary.top.terms %>%
  mutate(term = reorder_within(term, beta, topic)) %>%
  ggplot(aes(beta, term, fill = factor(topic))) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~ topic, scales = "free") +
  scale_y_reordered()
```

```{r}
diary_pre_1863 <- diary_whole_unnest %>% filter(year < 1863)

diary.dtm <- diary_pre_1863 %>% 
  count(file_name, word) %>% 
  cast_dtm(file_name, word, n)

diary.lda <- LDA(diary.dtm, k = 10, control = list(seed = 12345))
diary.lda

diary.topics <- tidy(diary.lda, matrix = "beta")
head(diary.topics)

diary.top.terms <- diary.topics %>%
  arrange(desc(beta)) %>% 
  group_by(topic) %>% slice(1:5)


diary.top.terms %>%
  mutate(term = reorder_within(term, beta, topic)) %>%
  ggplot(aes(beta, term, fill = factor(topic))) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~ topic, scales = "free") +
  scale_y_reordered()
```

```{r}
diary_post_1863 <- diary_whole_unnest %>% filter(year >= 1863)

diary.dtm <- diary_post_1863 %>% 
  count(file_name, word) %>% 
  cast_dtm(file_name, word, n)

diary.lda <- LDA(diary.dtm, k = 10, control = list(seed = 12345))
diary.lda

diary.topics <- tidy(diary.lda, matrix = "beta")
head(diary.topics)

diary.top.terms <- diary.topics %>%
  arrange(desc(beta)) %>% 
  group_by(topic) %>% slice(1:5)


diary.top.terms %>%
  mutate(term = reorder_within(term, beta, topic)) %>%
  ggplot(aes(beta, term, fill = factor(topic))) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~ topic, scales = "free") +
  scale_y_reordered()
```

```{r}
devtools::install_github("bmschmidt/wordVectors")
library(wordVectors)
library(tidyverse)
```

```{r}
prep_word2vec(origin="text files",destination="textfiles.txt",lowercase=T,bundle_ngrams=1)
```

```{r}
model = train_word2vec("textfiles.txt","textfiles.bin",vectors=400,threads=6,window=12,iter=3,negative_samples=5)

#play with vectors numbers and window (try going smaller and larger)
#also look at iterations
```

```{r}
model %>% closest_to("negroe")
```

```{r}
wowords <- model[[c("female", "females", "women", "woman", "feminine", "she", "woman's")]] %>% reject(model[[c("male", "males", "men", "man", "masculine", "he", "men's")]])
model %>% nearest_to(wowords, 100)

```

```{r}
mewords <- model[[c("male", "males", "men", "man", "masculine", "he", "men's", "man's")]] %>% reject(model[[c("female", "females", "women", "woman", "feminine", "she", "woman's", "women's")]])
model %>% nearest_to(mewords, 100)
```

```{r}
yankee_words = closest_to(model,model[[c("yankee","federal","evacuate", "officer")]],150)
yankee = model[[yankee_words$word,average=F]]
plot(yankee,method="pca")
```

```{r}
set.seed(10)
centers = 150
clustering = kmeans(model,centers=centers,iter.max = 40)


```

```{r}
sapply(sample(1:centers,10),function(n) {
  names(clustering$cluster[clustering$cluster==n][1:10])
})
```

```{r}
people = c("mother","father","yankee","negroe")
term_set = lapply(people, 
       function(people) {
          nearest_words = model %>% closest_to(model[[people]],20)
          nearest_words$word
        }) %>% unlist
subset = model[[term_set,average=F]]
subset %>%
  cosineDist(subset) %>% 
  as.dist %>%
  hclust %>%
  plot
```

```{r}
side = model[[c("confederacy","yankee"),average=F]]
# model[1:3000,] here restricts to the 3000 most common words in the set.
confederacy_union = model[1:3000,] %>% cosineSimilarity(side)
# Filter to the top 20 sweet or salty.
confederacy_union = confederacy_union[
  rank(-confederacy_union[,1])<20 |
  rank(-confederacy_union[,2])<20,
  ]
plot(confederacy_union,type='n')
text(confederacy_union,labels=rownames(confederacy_union))
```
